#!/usr/bin/env python3"""Reuses Battery Lite data processing to validate and transform EVSE Lite data.Enel X is our only EVSE Lite provider so far but code is generalized to support others."""import enrollment.evse_lite.m_and_v.model as mfrom datetime import datetime, timedeltafrom ehutils import ehlog, mongo_connfrom enrollment.lite_etl import LiteEtl, TargetField, our_uuid_from_theirs, to_datetime, to_float, to_bool, parse,MalformedFileErrorlog = ehlog.getLogger("enrollment.evse_lite.m_and_v.etl")class EvseLiteEtl(LiteEtl):    _PROJECT = 'evse_lite'    _COLL_NAME = "thermostatEvent.evChargerLog"    _HEADERS_TO_FIELDS = {        'evse uuid': TargetField(            m.UUID,            required=True        ),        'timestamp': TargetField(            m.STATION_TIME,            transform_fn=to_datetime,            required=True        ),        'evse peak power': TargetField(            m.PEAK_POWER,            transform_fn=to_float,            required=False        ),        'evse average power': TargetField(            m.POWER_AVERAGE,            transform_fn=to_float,            required=False        ),        'minutes charging': TargetField(            m.MINUTES_CHARGING,            transform_fn=to_float,            # We anticipate some values of 'N/A' potentially            required=False        ),        'evse mode': TargetField(            m.MODE        ),        'connectivity status': TargetField(            m.CONNECTIVITY_STATUS,            transform_fn=to_bool        )    }    def _get_mongo_db(self, read_only):        return mongo_conn.get_mongo_events_db(read_only=read_only)    def _get_session_id(self, dt, uuid):        unique_string = '{}.{}'.format(dt.strftime('%Y%m%d'), uuid)        # make the hash smaller to not lose precision when coerced to double in the Java;        # Java doubles follow IEEE 754 and have 53 bits of precision, so as long as we stay        # below 2**53 we should be ok.        #        # https://docs.oracle.com/javase/specs/jls/se7/html/jls-4.html#jls-4.2.3        #        # Unfortunately, Python doesn't guarantee that the `float` is 64-bit.        #        # see EH-31208 and net.energyhub.edx.datamodel.mongo.EvChargerLog#sessionId        return hash(unique_string) % 2**53    def _transform(self, provider, reader, error_allowance, count):        rows, count = super(EvseLiteEtl, self)._transform(provider, reader, error_allowance, count)        date_docs = {}        dates_missing_values = set()        rows_missing_values_count = 0        for r in rows:            r_keys = r.keys()            r_time = r["stationTime"]            date = r_time.date()            now_utc = datetime.now()            # Don't return entries without power data            if date in dates_missing_values or 'peakPowerInKW' not in r_keys or 'rollingPowerAvgInKW' not in r_keys:                rows_missing_values_count += 1                dates_missing_values.add(date)                if date in date_docs.keys():                    rows_missing_values_count += len(date_docs[date]["body"]["fifteenMinuteData"])                    del date_docs[date]                continue            # Don't need to store UUIDs in nested data            uuid = r.pop("thermostatUuid")            # Scale "hour-ized" measure to 15-minutes            r["energyConsumedInKWh"] = r["rollingPowerAvgInKW"] / 4            if date_docs.get(date, None):                if our_uuid_from_theirs(uuid, provider=provider) != doc["thermostatUuid"]:                    raise MalformedFileError("Data file contains more than one unique UUID")                body = date_docs[date]["body"]                if r_time < body["sessionStartTime"]:                    body["sessionStartTime"] = r_time                if r_time > body["sessionEndTime"]:                    body["sessionEndTime"] = r_time                    body["energy"] = r["energyConsumedInKWh"]                    body["sessionId"] = self._get_session_id(r_time, uuid)                body["sessionEnergy"] += r["energyConsumedInKWh"]                body["fifteenMinuteData"].append(r)            else:                doc = {"thermostatUuid": our_uuid_from_theirs(uuid, provider=provider),                       "nowUtc": now_utc,                       "body": {                           "sessionStartTime": r_time,                           "sessionEndTime": r_time,                           "timestamp": now_utc,                           "energy": r["energyConsumedInKWh"],                           "sessionId": self._get_session_id(r_time, uuid),                           "sessionEnergy": r["energyConsumedInKWh"],                           "fifteenMinuteData": [r]                       }}                date_docs[date] = doc        for date in sorted(date_docs.keys()):            d=date_docs[date]            # Set end time to last second of day            d['body']['sessionEndTime'] = d['body']['sessionStartTime'] + timedelta(days=1) - timedelta(seconds=1)            if len(d["body"]["fifteenMinuteData"]) != 96:                raise MalformedFileError(                    "Must have 96 intervals of valid data per day: thermostatUuid {}, sessionStartTime {}".format(                        d["thermostatUuid"],                        d["body"]["sessionStartTime"]))        log.info("Issues processing {} rows in this file".format(rows_missing_values_count))        return list(date_docs.values()), count - rows_missing_values_count    def _load(self, db, transformed, commit):        for t in transformed:            filter = {'thermostatUuid': t['thermostatUuid'],                      'body.sessionStartTime': t['body']['sessionStartTime']}            db[self._COLL_NAME].delete_one(filter)            # This will generate a new ID/creation time for the doc, which we use in downstream pipelines            # Risk of reading data during swap resolved by buffer for Mongo eventual consistency. See oakheart repo            db[self._COLL_NAME].insert_one(t)def main(providers, max_allowed_errors, commit, local_dir):    EvseLiteEtl().process(providers, max_allowed_errors, commit, local_dir)if __name__ == "__main__":    main(**vars(parse()))