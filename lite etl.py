#!/usr/bin/env python3"""Downloads files from brick, performs a bit of validation on their data,    and puts their data on mongo.The basic structure of this is very similar to    commercial_aggregator/validate_and_transform.py"""import argparseimport attrimport boto3import csvimport ioimport itertoolsimport osimport subprocessimport tempfilefrom datetime import datetimefrom ehutils import assets_bucket, brickftp, ehlog, ehubrcfrom ehutils.constants.ehub_constants import PlatformVersionlog = ehlog.getLogger("ehub.enrollment.lite_etl")STRING_TYPES = (str,)try:    STRING_TYPES = (str, unicode)except NameError:    passNULL_VALUES = {"N/A", "#N/A", "#VALUE!"}_BRICK_PATHS_BY_PROVIDER = {    PlatformVersion.PIKA.name: ["PikaEnergy/nationalgrid/from_pika/data"],    PlatformVersion.SOLAR_EDGE.name: ["SolarEdge/NationalGrid/from_SolarEdge/data"],    PlatformVersion.ENEL_X.name: [        "EnelX/BGE EV/from_EnelX/data",        "EnelX/Potomac/Off Peak Rewards/from_EnelX/data",        "EnelX/Eversource/from_enelX/data",        "EnelX/CPS EVSE/From Enel X/Data/Demand Response Program",        "EnelX/CPS EVSE/From Enel X/Data/Peak Time Rebate Program"    ],    PlatformVersion.SONNEN.name: ["Sonnen/NationalGrid/from_sonnen/data"],    PlatformVersion.SUNRUN.name: ["Sunrun/nationalgrid/From_Sunrun/data"],    PlatformVersion.TESLA.name: ["Tesla/nationalgrid/from_tesla/data"],    PlatformVersion.VIVINT_BATTERY.name: ["vivint/nationalgrid/from_vivint/data"]}_DATE_FORMATS = ("%Y-%m-%d %H:%M:%S", "%Y-%m-%d %H:%M", "%m/%d/%y %H:%M",                 "%m/%d/%Y %H:%M")_ASSETS_BUCKET_NAME = assets_bucket.AssetsBucket.get_instance().bucket_namedef our_uuid_from_theirs(their_uuid, **kwargs):    provider = kwargs.pop("provider")    if not provider:        raise ValueError("expected provider kwarg")    return "{}_{}".format(provider, their_uuid).lower()def to_bool(bool_str, **_):    if bool_str.lower() == "true":        return True    elif bool_str.lower() == "false":        return False    else:        raise InvalidValueError("{} is not TRUE or FALSE".format(bool_str))def to_datetime(date_str, **_):    for fmt in _DATE_FORMATS:        try:            return datetime.strptime(date_str, fmt)        except ValueError:            pass    raise InvalidValueError("date was in invalid format: " + date_str)def identity(x, **_):    return xdef callable_validator(_instance, _attribute, fn):    """    Can't do isinstance for functions, so we need a custom validator here    """    if not callable(fn):  # indicates programming error, not data error        raise TypeError("expected a callable, got a " + str(type(fn)))def to_float(float_str, **_):    return float(float_str)class InvalidValueError(Exception):    passclass MalformedFileError(Exception):    passclass LiteEtl(object):    _PROJECT = 'lite_etl'    _COLL_NAME = str()    _HEADERS_TO_FIELDS = dict()    def _validate_and_transform_row(self, row, provider):        """        Calls the validation function and the transform function            to get the target value        """        def lower_fields(d):            return [field_name.lower() for field_name in d.keys()]        header_set = set(lower_fields(row))        expected_headers = set(lower_fields(self._HEADERS_TO_FIELDS))        unknown_headers = header_set - expected_headers        missing_headers = expected_headers - header_set        if unknown_headers:            raise MalformedFileError("Encountered unfamiliar columns: {}".format(                unknown_headers))        if missing_headers:            raise MalformedFileError("Missing columns: {}".format(missing_headers))        transformed_row = dict()        for field_name, value in row.items():            lower_field = field_name.lower()            value = value.strip()            target_field = self._HEADERS_TO_FIELDS[lower_field]            if not target_field.is_valid(value, provider=provider):                # this should be enough info to find the offending row / file                raise InvalidValueError(                    "Encountered an invalid value for field: {{{}: {}}}, ".format(lower_field, value))            transformed_value = target_field.transform(value, provider=provider)            if transformed_value is not None:                transformed_row[target_field.field] = transformed_value        return transformed_row    def _transform(self, provider, reader, error_allowance, count):        transformed_rows = list()        num_errors = 0        for row in reader:            try:                transformed_row = self._validate_and_transform_row(row, provider)                transformed_rows.append(transformed_row)            except InvalidValueError as e:                num_errors += 1                log.warning(e)            count += 1        if num_errors > error_allowance:            raise InvalidValueError("hit {} errors, max {} allowed".format(num_errors, error_allowance))        return transformed_rows, count    def _get_mongo_db(self, read_only):        pass    def _load(self, db, transformed, commit):        pass    def _backup_to_s3(self, provider_file, provider, source_path):        provider_file.seek(0)        s3_client = boto3.client('s3')        dest_path = os.path.join("{}/data".format(self._PROJECT),                                 provider,                                 os.path.basename(source_path))        file_in_bytes = io.BytesIO(bytes(provider_file.read(), encoding='utf-8'))        bytes_reader = io.BufferedReader(file_in_bytes)        s3_client.upload_fileobj(bytes_reader, _ASSETS_BUCKET_NAME, dest_path)    def process(self, providers, max_allowed_errors, commit, local_dir):        error_allowance = max_allowed_errors        file_errors = []        count = 0        if local_dir:            file_handler = LocalFileHandler()        else:            file_handler = SftpFileHandler()        for provider in providers:            source = local_dir if local_dir else provider            for local_file, filepath in file_handler.get_files_for_provider(source):                reader = csv.DictReader(local_file)                try:                    transformed, count = self._transform(provider, reader, error_allowance, count)                    if commit:                        db = self._get_mongo_db(not commit)                        self._load(db, transformed, commit)                        self._backup_to_s3(local_file, provider, filepath)                        file_handler.remove_committed_file(filepath)                        log.info("successfully loaded %s rows total into the database so far", count)                    else:                        log.info("successfully validated %s rows total so far", count)                except (InvalidValueError, MalformedFileError) as e:                    # Track issues to report at end of pipeline                    log.error("This file had an issue. Proceeding to next file. "                              "Check end of log for all failing files.")                    file_errors.append("File at '{}' had issues: {}".format(filepath, str(e)))                    continue                finally:                    file_handler.handle_tmp_file(local_file)        # Report issues after processing all well-formatted files        for e in file_errors:            log.error(e)        if file_errors:            raise MalformedFileError("One or more files didn't meet standards. "                                     "Successfully processed {} rows in total".format(count))class FileHandler(object):    def get_files_for_provider(self, source):        pass    def remove_committed_file(self, filepath):        pass    def handle_tmp_file(self, local_file):        # Local file is only tmp if using SFTP        passclass SftpFileHandler(FileHandler):    def _normalize_text_encoding(self, open_file):        """        Remove BOM and carriage returns        :type open_file: file        """        # before these subprocess calls, we need to close the file        open_file.close()        # first, remove the byte-order marker if present (in place)        # https://unix.stackexchange.com/questions/381230/how-can-i-remove-the-bom-from-a-utf-8-file        subprocess.check_call(["sed", "-i", r"1s/^\xef\xbb\xbf//", open_file.name])        # remove carriage returns        subprocess.check_call(["sed", "-i", "-e" r"s/\r//g", open_file.name])        # reopen it for downstream consumers        return open(open_file.name, 'r')    def get_files_for_provider(self, source):        sftp = brickftp.get()        brickftp_root = ehubrc.lookup(["brickftp", "brickftp_root"]) or ''        provider_dirs = (            os.path.join(brickftp_root, path)            for path in _BRICK_PATHS_BY_PROVIDER[source]        )        dir_contents = (self._get_dir_contents(sftp, dirname) for dirname in provider_dirs)        return itertools.chain.from_iterable(dir_contents)    def _get_dir_contents(self, sftp, dirname):        for remote_file in sftp.listdir(dirname):            remote_path = os.path.join(dirname, remote_file)            with tempfile.NamedTemporaryFile(delete=False) as f:                sftp.getfo(remote_path, f)                f = self._normalize_text_encoding(f)                log.info("yielding file, remote: %s, local: %s", remote_path, f.name)                yield f, remote_path    def remove_committed_file(self, filepath):        sftp = brickftp.get()        sftp.remove(filepath)    def handle_tmp_file(self, local_file):        log.info("removing local tmp file %s", local_file.name)        os.remove(local_file.name)class LocalFileHandler(FileHandler):    def get_files_for_provider(self, source):        for filename in os.listdir(source):            filepath = os.path.join(source, filename)            with open(filepath) as file:                log.info("yielding file, local: %s", filepath)                yield file, filepath    def remove_committed_file(self, filepath):        os.remove(filepath)@attr.sclass TargetField(object):    field = attr.ib(validator=attr.validators.optional(        attr.validators.instance_of(STRING_TYPES)))    _validation_fn = attr.ib(validator=callable_validator,                             default=lambda x: True)    _transform_fn = attr.ib(validator=callable_validator,                            default=identity)    _required = attr.ib(validator=attr.validators.instance_of(bool),                        default=False)    _validate_by_transform = attr.ib(        validator=attr.validators.instance_of(bool), default=True)    def is_valid(self, value, **kwargs):        if value is None or value == "":            raise InvalidValueError("Value was empty for field: {}".format(                self.field))        if self._required and value in NULL_VALUES:            raise InvalidValueError("Got a NULL_VALUE for a required field: {}"                                    .format(self.field))        if self._validate_by_transform:            # noinspection PyBroadException            try:                self.transform(value, **kwargs)                return True            except Exception:                return False        return self._validation_fn(value)    def transform(self, value, **kwargs):        if value in NULL_VALUES:            return None        return self._transform_fn(value, **kwargs)def parse():    def ensure_valid_provider(provider):        provider_upper = provider.upper()        if provider_upper not in _BRICK_PATHS_BY_PROVIDER.keys():            raise argparse.ArgumentTypeError("invalid provider: {}"                                             .format(provider))        return provider_upper    parser = argparse.ArgumentParser(description=__doc__)    parser.add_argument("--provider",                        dest="providers",                        default=[],  # only way to get the append working                        action="append",                        type=ensure_valid_provider,                        help="space-separated list of providers to load")    parser.add_argument("--max-allowed-errors", type=int, default=25)    parser.add_argument("--commit", action="store_true",                        help="whether to load the data into the database")    parser.add_argument("--local_dir",                        type=str,                        help="local directory with data files. uses sftp source for provider(s) if local_dir not specified")    args = parser.parse_args()    if not args.providers:        args.providers = _BRICK_PATHS_BY_PROVIDER.keys()    return args